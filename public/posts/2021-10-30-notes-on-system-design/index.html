<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes on System Design | Random notes to myself</title>
<meta name=keywords content="system design,cache,high load"><meta name=description content="These are hectic notes on a book &ldquo;System design interview&rdquo; by Alex Xu.
chapter 1
read-through cache strategy — check if we have a response cached, if we do return it, if not then fetch data from a database, cache it and return result
GeoDNS is a patch for BIND DNS server software that allows for geographical split horizon
A few words about caching strategy. Your caching strategy depends on your data access patterns."><meta name=author content><link rel=canonical href=http://localhost:1313/posts/2021-10-30-notes-on-system-design/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2021-10-30-notes-on-system-design/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Notes on System Design"><meta property="og:description" content="These are hectic notes on a book &ldquo;System design interview&rdquo; by Alex Xu.
chapter 1
read-through cache strategy — check if we have a response cached, if we do return it, if not then fetch data from a database, cache it and return result
GeoDNS is a patch for BIND DNS server software that allows for geographical split horizon
A few words about caching strategy. Your caching strategy depends on your data access patterns."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2021-10-30-notes-on-system-design/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-10-30T17:29:38+03:00"><meta property="article:modified_time" content="2021-10-30T17:29:38+03:00"><meta property="og:site_name" content="Random notes to myself"><meta name=twitter:card content="summary"><meta name=twitter:title content="Notes on System Design"><meta name=twitter:description content="These are hectic notes on a book &ldquo;System design interview&rdquo; by Alex Xu.
chapter 1
read-through cache strategy — check if we have a response cached, if we do return it, if not then fetch data from a database, cache it and return result
GeoDNS is a patch for BIND DNS server software that allows for geographical split horizon
A few words about caching strategy. Your caching strategy depends on your data access patterns."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Notes on System Design","item":"http://localhost:1313/posts/2021-10-30-notes-on-system-design/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes on System Design","name":"Notes on System Design","description":"These are hectic notes on a book \u0026ldquo;System design interview\u0026rdquo; by Alex Xu.\nchapter 1\nread-through cache strategy — check if we have a response cached, if we do return it, if not then fetch data from a database, cache it and return result\nGeoDNS is a patch for BIND DNS server software that allows for geographical split horizon\nA few words about caching strategy. Your caching strategy depends on your data access patterns.","keywords":["system design","cache","high load"],"articleBody":"These are hectic notes on a book “System design interview” by Alex Xu.\nchapter 1\nread-through cache strategy — check if we have a response cached, if we do return it, if not then fetch data from a database, cache it and return result\nGeoDNS is a patch for BIND DNS server software that allows for geographical split horizon\nA few words about caching strategy. Your caching strategy depends on your data access patterns. Is your system write heavy, read heavy, results are always should be unique ?\nCaching strategies:\ncache-aside — cache sits “aside” and application interacts directly with cache and database layers. Most common approach read-through cache — an applications reads through cache, in other words application interacts with cache only. When there is a cache miss, cache itself requests data from database, caches it and returns response write-through cache — first data is written to the cache and then to the database. Writes are going through cache and to the database write-around — writes are going to the database and only reads are saved in cache write-back — writes are coming to cache layer and immediately acknowledge. Cache layer, later on, write to the database Caching strategies\nchapter 2\nCommon back-of-the-envelope estimation questions: qps, peak qps, cache, number of servers and etc.\nLatency Numbers Every Programmer Should Know: chapter 3\n4 steps for effective system design interview\nunderstand the problem and establish design scope. Get all the basic params and features of a system to be designed propose high-level desing and get an appoval for it, so you can move forward with design deep dive into details — identify bottlenecks, system performance, resource estimation wrap up, propose some other possible solutions, talk to your interviewer etc.. chapter 4\nRate limiting algorithms:\ntoken bucket leaking bucket fixed window counter sliding window log sliding window counter chapter 5\nConsistent Hashing\nWhat is it?\nA technique called hash ring is used to implement consistent hashing.\nA ring is an array, which end is connected to the beginning, thus making it a ring:\nhash servers S0, S1, S2, S3 9, 1, 2, 3, 4, 5 are cache keys that are assigned to S0 server 6, 7 keys that are assigned to S1 so on.. ┌────────────────────────────────────────────────────────────┐ ▼ │ start end ───────────S0───────────S1──────────────S2────────────S3────── 1,2,3,4,5 6,7 8 11 If it is a ring, then moving clock wise from key value will give you server it is assigned to.\nA techinique called virtual nodes helps to spread keys evently, in cases then servers are added or removed. A bunch of virtual nodes represent a cache server, and virtual nodes are evently spread on the ring.\nchapter 6\nkey-value store\nEven with data compression or/and swapping less frequently used data to disc, a single server implementation will reach capacity very quickly handling big data. The answer to this is a distributed key-value store, also known as distributed hash table.\nWhen designing a distributed system it is important to know about CAP theorem. CAP stands for:\nConsistency. All nodes are synced, user always receives correct response no matter what node used connected to Availability. Even if some nodes are down, client always gets a response Partition tolerance. System continues to operate even if some of its nodes are down — network partitions Theorem states, that you can have any two of features above, but not all of them together.\nThus you can have:\nCP, Consistency + Partition tolerance AP, Availability + Partition tolerance CA, Consistenty + Availability In a real world network partitions will happen, so Consitency + Availability combination can not happen in real world because you always need Partition tolerance. So that leaves us with wether CP or AP. When discussing design approach you either may choose availability, which means stale reads but working writes, or you can choose consitency, which means no writes but correct reads.\nBuilding blocks of a system:\ndata partition. Use consistent hashing, which gives you equal distrinbution of data between servers and also gives you heterogenity, meaning a server with bigger capacity will have more virtual nodes placed on a hash ring data replication, replication might be done by just replicating to the first 3 nodes you find on hash ring moving clock wise consistency, with number of servers (N), writes quorum (W) and read quorum (R) you can tune your system parameters as needed. e.g. R=1, W=3 means system is designed for fast reads — “coordinator” should get response from only 1 node. consistency modes are strong consistency, weak consistency and eventual consistency inconsistency resolution — versioning, vector clocks Failure is inevitable in a distributed system. For failure detection — use gossip protocol.\nAdditional notes for the chapter.\nLamport timestamps.\nKey idea is: each node and each client keeps track of maximum value it has seen so far and includes that value on every request.\neach node tracks a number of operation it processed each node has a unique identifier each node and every client includes its own version of max value they have seen so far value with greater counter value is bigger if counter values are equal then node with greated node id is bigger timestamp Lamport timestamp: (counter, node id) Max value 0 1 4 5 Client 1 ───────▲────────────────────────▲───────────▲──────► │ │ │ A│ E│ G│ │ │ │ Node 1 ───────▼────────────────────────┼───────────▼──────► Timestamp (0,1) (1,1) │ (5,1) │ │ Timestamp (0,2) (1,2) (2,2) (3,2) │ (4,2) (5,2) Node 2 ───────▲──────▲──────▲──────────▼──────▲───────────► │ │ │ │ B│ C│ D│ F│ │ │ │ │ Client 2 ───────▼──────▼──────▼─────────────────▼───────────► Max value 0 1 2 3 5 todo: like… find an example, come up with example yourself\nchapter 7\nDesign unique ID generator\nRequirement:\nsortable by time 64 bits length unique numeric only UUID is a great solution, but it is 128 bits length. For us Twitter Showflake ID generator fits perfectly.\nIt is simple and efficient and looks like this:\n┌─┬─────────────────────────┬───────────────┬────────────┬───────────────────┐ │0│ timestamp │ datacenter ID │ machine ID │ sequence number │ └─┴─────────────────────────┴───────────────┴────────────┴───────────────────┘ ▲ ▲ ▲ ▲ ▲ 1 bit│ 41 bits │ 5 bits │ 5 bits │ 12 bits │ ─────┴─────────────────────────┴───────────────┴────────────┴───────────────────┘ First bit is not used and reserved for future uses. Interestingly, sequence number is reset to 0 every millisecond.\nchapter 8\nURL shortener\nTypes of redirect — 301 and 302 http statuses. 301 means permanent redirect, which means browser will cache it and next time will hit long url directly. 302 meand temporary redirect so that the browser hits short url every time.\n301 reduces server load 302 allows to have better tracking and analytics\nTwo types of hash functions along with collision detection are mentioned. First one is to use well known hash function MD5, SHA-1 and etc. to calculate hash. If there is a collision we might approach it the following way:\ngenerate hash for the long url check if database has this hash key if yes then append some predefined string to long url and generate it again do prev step recursively until collision is resolved Method above works, but it might be expensive to always fetch from database in order to know if record is already exists — collision exists. In this situation a bloom filter is an elegant solution for this problem.\nSecond method is base 62 conversion. base 62 is a way to use 62 characters for encoding, that is 0-0, 1-1, 10-a, 11-b, 35-z, 36-A, 61-Z. Leaving us with “alphabet” that looks like this “0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ”\nchapter 9\nWeb crawler\nnothing notable (?), I probably need to review it one more time.\nchapter 10\nNotification system\nnothing notable (?), I probably need to review it one more time.\nchapter 11\nNews feed system\nFeed system includes fanout service. That is when a fried, e.g. posts something to the feed, all frieds should receive/see that post in their feeds. For this fanout service is used. This kind of service delivers data to all interested parties. There are two types of fanout services: fanout on write and fanout on read.\nFanout on write, writes/updates feed of all the friends/people interested in. Pros: updates feeds immediately, that is feeds are being pre-computed and feed delivery is fast. Cons: pre-computation takes time and might be inefficient since some of the people are reading their feeds rarely. Pre-computation might take a lot of time when user have many friends — this is called hotkey problem.\nFanout on read means feed is updated when user requests it. Pros: no extra pre-computation required. Cons: feed request might be executing slowly.\nchapter 12\nChat system\nWays to simulate full-duplex connection: polling, long polling.\nPolling is simple and a well-known techinque — you just ask server periodically about new messages. Client opens connection, asking for any info for it, and closes it. And then again opens it. Long polling is a bit more complex — client keeps connection open until server does have some info/messages. Client gets the messages, closes connection and immediately opens up a new connection.\nWeb socket is the most common solution to establish full-duplex connection. WS uses HTTP connection to establish connection between client and server. When connection is established, WS does protocol switch from HTTP to web socket protocol.\nStorage. For user profile, settings, friend list and etc. a robust relational database is a good solution. For chat messages, when users access only recent chat story, a key-value storage is a optimal solution. Facebook messenger uses HBase storage and Discord uses Cassandra.\nchapter 13\nAutocomplete system\nCan be made of Data gathering service and Data query service. Data gathering happens as user types search queries. Search queries are being saved into simple query frequency table which is a key-value table where key is a search term and values is the number of times user did query. Data query service basically returns all matching keys from frequency table. e.g. “tw” would match and return terms like “twitter”, “twitch”, “two” (terms are sorted by desc search frequency)\nIt works for a smaller system, but it does not scale very much. Trie data structure is needed for a mature, more fit solution to autocomplete system.\nEach node in a trie, we are going to use, should also have a frequency number (weight) which is used to get top k queries by given prefix. We can also cache top k search queries for every node.\nchapter 14\nDesign Youtube\nVideo streaming flow. Streaming protocol — is a standardized way to control data transfer for video streaming. There are a few streaming protocols to chose from: MPEG-DASH, Apple HLS, microsoft smooth streaming, Adobe HDS.\nThere are many types of encoding formats, but most of them consist of two parts — container and codec. You can tell container by file extension, e.g. .avi, .mpeg, .mkv and etc. A container is like a box that contains video, audio and metadata altogether. Codec is a compression and decompression algorithm that should reduce size of a files in container not loosing any quality.\nSafety optimization — pre-signed URL. Client, first, does request to e.g. /upload endpoint that does auth checks and returns pre-signed URL, which client uses to upload a file. Amazon S3 call is pre-signed URL, while Microsoft Asure calls it Shared Access Signature.\nCost saving optimization.\nNot everything should be stored in CDN. CDN on a large scale is expensive. We can store in CDN only the most popular videos. Less popular videos we can store in our own storage and for these videos we can make less kinds of video formats. Sometimes video is popular in a specific region only, so it makes less sense to store it globally. chapter 15\nGoogle drive\nOne of the notable features of any mature file sync system is that it must have a block server. Block server is able to split a file by blocks, e.g. Dropbox max block size is 6Mb. Every block is identified by hash value. A file can be re-assembled from these blocks. In order to understand which blocks to what file belong and in what order, block server should somehow save metadata information about original file and blocks. Typically metadata is being saved in some database.\nFiles that are accessed rarely might/should be moved to the “cold” storage. It is a typically much slower but cheaper type of storage.\n","wordCount":"2039","inLanguage":"en","datePublished":"2021-10-30T17:29:38+03:00","dateModified":"2021-10-30T17:29:38+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2021-10-30-notes-on-system-design/"},"publisher":{"@type":"Organization","name":"Random notes to myself","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=archives><span>archives</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Notes on System Design</h1><div class=post-meta>&lt;span title='2021-10-30 17:29:38 +0300 EEST'>October 30, 2021&lt;/span>&amp;nbsp;·&amp;nbsp;10 min&amp;nbsp;·&amp;nbsp;2039 words&nbsp;|&nbsp;<a href=https://github.com/slash3b/slash3b.github.io/content/posts/2021-10-30-notes-on-system-design.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>These are hectic notes on a book &ldquo;System design interview&rdquo; by Alex Xu.</p><p><em>chapter 1</em></p><ul><li><p><strong>read-through cache strategy</strong> — check if we have a response cached, if we do return it, if not then fetch data from a
database, cache it and return result</p></li><li><p><strong>GeoDNS</strong> is a patch for BIND DNS server software that allows for geographical split horizon</p></li></ul><p>A few words about caching strategy.
Your caching strategy depends on your data access patterns. Is your system write heavy, read heavy, results are always
should be unique ?</p><p>Caching strategies:</p><ul><li><strong>cache-aside</strong> — cache sits &ldquo;aside&rdquo; and application interacts directly with cache and database layers. Most common
approach</li><li><strong>read-through cache</strong> — an applications reads through cache, in other words application interacts with cache only.
When there is a cache miss, cache itself requests data from database, caches it and returns response</li><li><strong>write-through cache</strong> — first data is written to the cache and then to the database. Writes are going through cache
and to the database</li><li><strong>write-around</strong> — writes are going to the database and only reads are saved in cache</li><li><strong>write-back</strong> — writes are coming to cache layer and immediately acknowledge. Cache layer, later on, write to the
database</li></ul><p><a href=https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/>Caching strategies</a></p><hr><p><em>chapter 2</em></p><p>Common back-of-the-envelope estimation questions: qps, peak qps, cache, number of servers and etc.</p><p>Latency Numbers Every Programmer Should Know:
<script src=https://gist.github.com/jboner/2841832.js></script></p><hr><p><em>chapter 3</em></p><p>4 steps for effective system design interview</p><ul><li>understand the problem and establish design scope. Get all the basic params and features of a system to be designed</li><li>propose high-level desing and get an appoval for it, so you can move forward with design</li><li>deep dive into details — identify bottlenecks, system performance, resource estimation</li><li>wrap up, propose some other possible solutions, talk to your interviewer etc..</li></ul><hr><p><em>chapter 4</em></p><p>Rate limiting algorithms:</p><ul><li>token bucket</li><li>leaking bucket</li><li>fixed window counter</li><li>sliding window log</li><li>sliding window counter</li></ul><hr><p><em>chapter 5</em></p><p>Consistent Hashing</p><p>What is it?</p><p>A technique called hash ring is used to implement consistent hashing.</p><p>A ring is an array, which end is connected to the beginning, thus making it a ring:</p><ul><li>hash servers <code>S0</code>, <code>S1</code>, <code>S2</code>, <code>S3</code></li><li>9, 1, 2, 3, 4, 5 are cache keys that are assigned to S0 server</li><li>6, 7 keys that are assigned to S1</li><li>so on..</li></ul><pre tabindex=0><code>

     ┌────────────────────────────────────────────────────────────┐
     ▼                                                            │
     start                                                    end
     ───────────S0───────────S1──────────────S2────────────S3──────
      1,2,3,4,5      6,7            8               11
</code></pre><p>If it is a ring, then moving clock wise from key value will give you server it is assigned to.</p><p>A techinique called virtual nodes helps to spread keys evently, in cases then servers are added or
removed. A bunch of virtual nodes represent a cache server, and virtual nodes are evently spread on the ring.</p><hr><p><em>chapter 6</em></p><p>key-value store</p><p>Even with data compression or/and swapping less frequently used data to disc, a single server implementation will
reach capacity very quickly handling big data. The answer to this is a distributed key-value store, also known as
distributed hash table.</p><p>When designing a distributed system it is important to know about CAP theorem. CAP stands for:</p><ul><li>Consistency. All nodes are synced, user always receives correct response no matter what node used connected to</li><li>Availability. Even if some nodes are down, client always gets a response</li><li>Partition tolerance. System continues to operate even if some of its nodes are down — network partitions</li></ul><p>Theorem states, that you can have any two of features above, but not all of them together.</p><p>Thus you can have:</p><ul><li>CP, Consistency + Partition tolerance</li><li>AP, Availability + Partition tolerance</li><li>CA, Consistenty + Availability</li></ul><p>In a real world network partitions will happen, so Consitency + Availability combination can not happen in real
world because you always need Partition tolerance. So that leaves us with wether CP or AP. When discussing design
approach you either may choose availability, which means stale reads but working writes, or you can choose consitency,
which means no writes but correct reads.</p><p>Building blocks of a system:</p><ul><li>data partition. Use consistent hashing, which gives you equal distrinbution of data between servers and also gives you
heterogenity, meaning a server with bigger capacity will have more virtual nodes placed on a hash ring</li><li>data replication, replication might be done by just replicating to the first 3 nodes you find on hash ring moving
clock wise</li><li>consistency, with number of servers (N), writes quorum (W) and read quorum (R) you can tune your system parameters as
needed. e.g. R=1, W=3 means system is designed for fast reads — &ldquo;coordinator&rdquo; should get response from only 1 node.</li><li>consistency modes are strong consistency, weak consistency and eventual consistency</li><li>inconsistency resolution — versioning, vector clocks</li></ul><p>Failure is inevitable in a distributed system. For failure detection — use gossip protocol.</p><p>Additional notes for the chapter.</p><p>Lamport timestamps.</p><p>Key idea is: <strong>each node and each client keeps track of maximum value it has seen so far and includes that value on every request</strong>.</p><ul><li>each node tracks a number of operation it processed</li><li>each node has a unique identifier</li><li>each node and every client includes its own version of max value they have seen so far</li><li>value with greater counter value is bigger</li><li>if counter values are equal then node with greated node id is bigger timestamp</li></ul><pre tabindex=0><code>           Lamport timestamp: (counter, node id)

Max value    0             1                      4        5
Client 1   ───────▲────────────────────────▲───────────▲──────►
                  │                        │           │
                 A│                       E│          G│
                  │                        │           │
 Node 1    ───────▼────────────────────────┼───────────▼──────►
 Timestamp  (0,1)         (1,1)            │             (5,1)
                                           │
                                           │
 Timestamp  (0,2)  (1,2)   (2,2)   (3,2)   │ (4,2)   (5,2)
 Node 2    ───────▲──────▲──────▲──────────▼──────▲───────────►
                  │      │      │                 │
                 B│     C│     D│                F│
                  │      │      │                 │
Client 2   ───────▼──────▼──────▼─────────────────▼───────────►
Max value     0       1      2      3                  5
</code></pre><p>todo: like&mldr; find an example, come up with example yourself</p><hr><p><em>chapter 7</em></p><p>Design unique ID generator</p><p>Requirement:</p><ul><li>sortable by time</li><li>64 bits length</li><li>unique</li><li>numeric only</li></ul><p>UUID is a great solution, but it is 128 bits length. For us Twitter Showflake ID generator fits perfectly.</p><p>It is simple and efficient and looks like this:</p><pre tabindex=0><code>   ┌─┬─────────────────────────┬───────────────┬────────────┬───────────────────┐
   │0│       timestamp         │ datacenter ID │ machine ID │  sequence number  │
   └─┴─────────────────────────┴───────────────┴────────────┴───────────────────┘
     ▲                         ▲               ▲            ▲                   ▲
1 bit│        41 bits          │    5 bits     │  5 bits    │    12 bits        │
─────┴─────────────────────────┴───────────────┴────────────┴───────────────────┘
</code></pre><p>First bit is not used and reserved for future uses.
Interestingly, sequence number is reset to 0 every millisecond.</p><hr><p><em>chapter 8</em></p><p>URL shortener</p><p>Types of redirect — 301 and 302 http statuses.
301 means permanent redirect, which means browser will cache it and next time will hit long url directly.
302 meand temporary redirect so that the browser hits short url every time.</p><p>301 reduces server load
302 allows to have better tracking and analytics</p><p>Two types of hash functions along with collision detection are mentioned.
First one is to use well known hash function MD5, SHA-1 and etc. to calculate hash. If there is a collision
we might approach it the following way:</p><ul><li>generate hash for the long url</li><li>check if database has this hash key</li><li>if yes then append some predefined string to long url and generate it again</li><li>do prev step recursively until collision is resolved</li></ul><p>Method above works, but it might be expensive to always fetch from database in
order to know if record is already exists — collision exists. In this situation
a bloom filter is an elegant solution for this problem.</p><p>Second method is base 62 conversion.
base 62 is a way to use 62 characters for encoding, that is 0-0, 1-1, 10-a, 11-b, 35-z, 36-A, 61-Z.
Leaving us with &ldquo;alphabet&rdquo; that looks like this &ldquo;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ&rdquo;</p><hr><p><em>chapter 9</em></p><p>Web crawler</p><p>nothing notable (?), I probably need to review it one more time.</p><hr><p><em>chapter 10</em></p><p>Notification system</p><p>nothing notable (?), I probably need to review it one more time.</p><hr><p><em>chapter 11</em></p><p>News feed system</p><p>Feed system includes fanout service. That is when a fried, e.g. posts something to the feed, all frieds should
receive/see that post in their feeds. For this fanout service is used. This kind of service delivers data to all
interested parties. There are two types of fanout services: fanout on write and fanout on read.</p><p>Fanout on write, writes/updates feed of all the friends/people interested in. Pros: updates feeds immediately, that is
feeds are being pre-computed and feed delivery is fast. Cons: pre-computation takes time and might be inefficient since
some of the people are reading their feeds rarely. Pre-computation might take a lot of time when user have many friends
— this is called hotkey problem.</p><p>Fanout on read means feed is updated when user requests it. Pros: no extra pre-computation required. Cons: feed request might be executing slowly.</p><hr><p><em>chapter 12</em></p><p>Chat system</p><p>Ways to simulate full-duplex connection: polling, long polling.<br>Polling is simple and a well-known techinque — you just ask server periodically about new messages. Client opens
connection, asking for any info for it, and closes it. And then again opens it.
Long polling is a bit more complex — client keeps connection open until server does have some info/messages. Client
gets the messages, closes connection and immediately opens up a new connection.</p><p>Web socket is the most common solution to establish full-duplex connection. WS uses HTTP connection to establish
connection between client and server. When connection is established, WS does protocol switch from HTTP to web socket
protocol.</p><p>Storage.
For user profile, settings, friend list and etc. a robust relational database is a good solution. For chat messages,
when users access only recent chat story, a key-value storage is a optimal solution. Facebook messenger uses HBase
storage and Discord uses Cassandra.</p><hr><p><em>chapter 13</em></p><p>Autocomplete system</p><p>Can be made of Data gathering service and Data query service. Data gathering happens as user types search queries.
Search queries are being saved into simple query frequency table which is a key-value table where key is a search term
and values is the number of times user did query. Data query service basically returns all matching keys from frequency
table. e.g. &ldquo;tw&rdquo; would match and return terms like &ldquo;twitter&rdquo;, &ldquo;twitch&rdquo;, &ldquo;two&rdquo; (terms are sorted by desc search frequency)</p><p>It works for a smaller system, but it does not scale very much.
Trie data structure is needed for a mature, more fit solution to autocomplete system.</p><p>Each node in a trie, we are going to use, should also have a frequency number (weight) which is used to get top <em>k</em>
queries by given prefix. We can also cache top <em>k</em> search queries for every node.</p><hr><p><em>chapter 14</em></p><p>Design Youtube</p><p>Video streaming flow.
Streaming protocol — is a standardized way to control data transfer for video streaming. There are a few streaming
protocols to chose from: MPEG-DASH, Apple HLS, microsoft smooth streaming, Adobe HDS.</p><p>There are many types of encoding formats, but most of them consist of two parts — container and codec.
You can tell container by file extension, e.g. .avi, .mpeg, .mkv and etc. A container is like a box that contains video,
audio and metadata altogether. Codec is a compression and decompression algorithm that should reduce size of a files in
container not loosing any quality.</p><p>Safety optimization — pre-signed URL. Client, first, does request to e.g. <code>/upload</code> endpoint that does auth checks and
returns pre-signed URL, which client uses to upload a file. Amazon S3 call is pre-signed URL, while Microsoft Asure
calls it Shared Access Signature.</p><p>Cost saving optimization.</p><ul><li>Not everything should be stored in CDN. CDN on a large scale is expensive. We can store in CDN only the most popular videos.</li><li>Less popular videos we can store in our own storage and for these videos we can make less kinds of video formats.</li><li>Sometimes video is popular in a specific region only, so it makes less sense to store it globally.</li></ul><hr><p><em>chapter 15</em></p><p>Google drive</p><p>One of the notable features of any mature file sync system is that it must have a block server. Block server is able to
split a file by blocks, e.g. Dropbox max block size is 6Mb. Every block is identified by hash value. A file can be
re-assembled from these blocks. In order to understand which blocks to what file belong and in what order, block server
should somehow save metadata information about original file and blocks. Typically metadata is being saved in some
database.</p><p>Files that are accessed rarely might/should be moved to the &ldquo;cold&rdquo; storage. It is a typically much slower but cheaper
type of storage.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/system-design/>System Design</a></li><li><a href=http://localhost:1313/tags/cache/>Cache</a></li><li><a href=http://localhost:1313/tags/high-load/>High Load</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/2021-12-28-simple-case-of-golang-profiler-usage/><span class=title>« Prev</span><br><span>Simple case of profiler usage in golang</span>
</a><a class=next href=http://localhost:1313/posts/2021-09-04-go-patterns/><span class=title>Next »</span><br><span>Golang concurrency tricks and patterns</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Random notes to myself</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>