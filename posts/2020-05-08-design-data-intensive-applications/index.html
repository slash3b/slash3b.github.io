<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems | Random notes to myself</title><meta name=keywords content><meta name=description content="Designing Data-Intensive Applications by Martin Kleppmann is an amazing journey into the world of distributed systems. It is a must read, especially if you are like me — self-taught programmer. I think it will be usefull even if you do not plan to work with distributed systems, it is just interesting per se.
I really learned a lot, than you Martin!
I have a feeling that I managed to digest and remember around 5% of the information, and the book contains a lot of good stuff, I mean a lot."><meta name=author content><link rel=canonical href=https://slash3b.github.io/posts/2020-05-08-design-data-intensive-applications/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://slash3b.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://slash3b.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://slash3b.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://slash3b.github.io/apple-touch-icon.png><link rel=mask-icon href=https://slash3b.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems"><meta property="og:description" content="Designing Data-Intensive Applications by Martin Kleppmann is an amazing journey into the world of distributed systems. It is a must read, especially if you are like me — self-taught programmer. I think it will be usefull even if you do not plan to work with distributed systems, it is just interesting per se.
I really learned a lot, than you Martin!
I have a feeling that I managed to digest and remember around 5% of the information, and the book contains a lot of good stuff, I mean a lot."><meta property="og:type" content="article"><meta property="og:url" content="https://slash3b.github.io/posts/2020-05-08-design-data-intensive-applications/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-05-08T00:00:00+00:00"><meta property="article:modified_time" content="2020-05-08T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems"><meta name=twitter:description content="Designing Data-Intensive Applications by Martin Kleppmann is an amazing journey into the world of distributed systems. It is a must read, especially if you are like me — self-taught programmer. I think it will be usefull even if you do not plan to work with distributed systems, it is just interesting per se.
I really learned a lot, than you Martin!
I have a feeling that I managed to digest and remember around 5% of the information, and the book contains a lot of good stuff, I mean a lot."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://slash3b.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems","item":"https://slash3b.github.io/posts/2020-05-08-design-data-intensive-applications/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems","name":"Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems","description":"Designing Data-Intensive Applications by Martin Kleppmann is an amazing journey into the world of distributed systems. It is a must read, especially if you are like me — self-taught programmer. I think it will be usefull even if you do not plan to work with distributed systems, it is just interesting per se.\nI really learned a lot, than you Martin!\nI have a feeling that I managed to digest and remember around 5% of the information, and the book contains a lot of good stuff, I mean a lot.","keywords":[],"articleBody":"Designing Data-Intensive Applications by Martin Kleppmann is an amazing journey into the world of distributed systems. It is a must read, especially if you are like me — self-taught programmer. I think it will be usefull even if you do not plan to work with distributed systems, it is just interesting per se.\nI really learned a lot, than you Martin!\nI have a feeling that I managed to digest and remember around 5% of the information, and the book contains a lot of good stuff, I mean a lot. So for myself I decided to get table of book contents and augment it with little hints and notes that will be helpful in case I want to remember something quickly.\nComputing is pop culture. […] Pop culture holds a disdain for history. Pop culture is all about identity and feeling like you’re participating. It has nothing to do with cooperation, the past or the furute — it’s living in the present. I think the same is true of most people who write code for money. They have no idea where [their culture came from]. Alan Kay, in interview with Dr. Dobb’s journal(2012)\nTable of Contents\nI. Foundations of Data Systems\n1. Reliable, Scalable, and Maintainable Applications Thinking About Data Systems Reliability Hardware Faults Software Errors Human Errors How Important Is Reliability? Scalability Describing Load Describing Performance Approaches for Coping with Load Maintainability Operability: Making Life Easy for Operations Simplicity: Managing Complexity Evolvability: Making Change Easy Summary Highlights and exerpts: Many applications today are data-intensive and not compute-intensive. Scalability is the term we use to describe a system’s ability to cope with increased load. Load paramenters — e.g. requests per second, number of simultaneous user online, the ration of reads to writes in a database. Percentiles — e.g. with a sorted list of responses, you can see that if the 95th percentile response time is 1.5 seconds, that means 95 out of 100 requests take less than 1.5 seconds and 5 out of 100 requests take 1.5 seconds or more. Percentiles are used in Service Level Objectives and Service Level Agreements.\nMaking a system simpler does not necessarily mean reducing its functionality; it can also mean removing accidental complexity. Reliability means making systems work correctly, even when faults occur. Scalability means having strategies for keeping performance good, even when load increases. Maintainability it’s about making life better for the engineering and operations teams who need to work with the system. 2. Data Models and Query Languages Relational Model Versus Document Model The Birth of NoSQL The Object-Relational Mismatch Many-to-One and Many-to-Many Relationships Are Document Databases Repeating History? Relational Versus Document Databases Today Query Languages for Data Declarative Queries on the Web MapReduce Querying Graph-Like Data Models Property Graphs The Cypher Query Language Graph Queries in SQL Triple-Stores and SPARQL The Foundation: Datalog Summary Highlights and exerpts: Historically data was represented as a tree. Later on with neccessity to represent relations between records relation model was invented. Today we have tree “main directions” to store data:\n“relational” — in case you have a relations between records “graph” covers cases when everything is related to everything, think Twitter connections between people “document” good when you have a fewer relations between documents or none at all 3. Storage and Retrieval Data Structures That Power Your Database Hash Indexes SSTables and LSM-Trees B-Trees Comparing B-Trees and LSM-Trees Other Indexing Structures Transaction Processing or Analytics? Data Warehousing Stars and Snowflakes: Schemas for Analytics Column-Oriented Storage Column Compression Sort Order in Column Storage Writing to Column-Oriented Storage Aggregation: Data Cubes and Materialized Views Summary Highlights and exerpts: This chapter tries to explain how databases handle storage and retrieval.\nStorage engines can be divided in two categories:\noptimized for transaction processing — OLTP, a.k.a. Online Transaction Processing. This is where typical CRUD lives and thrives. optimized for analytics — OLAP, a.k.a. Online Analytical Processing. OLTP can be divided in two sub-categories:\nlog-structured append-only model with clean up of obsolete files. Examples: Bitcask, SSTables, LSM-trees, LevelDB, Cassandra, HBase, Lucene and etc. update-in-place model, which treats the disk as a set of fixed-size pages than can be updated. Log-structured storage engines are a comparatively recent development. Their key idea is that they systematically turn random-access writes into sequential writes on disk, which enables higher write throughtput due to performance characteristics of hard drives and SSDs.\nOther notes: Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.\nA database index is an additional structure, traditionally — B-tree, that has to be updated when we write to the database. Hence we have a tradeoff here, namely — index speed up read requests but slows down write requests. Bitcast is the default storage engine of Riak, it is roughtly an append-only storage that tracks every record offset. Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.\nIt … seems likely that in ther foreseeable future relational databases will continue to be used alongside a broad viriety of nonrelational datastore — an idea that is called polyglot persistence.\n… However if your application does use many-to-many relations, the document model becomes less appealing.\nWhen you most likely have many interconnected entities, where everything migh relate to everything — graph database is a good choice.\n4. Encoding and Evolution Formats for Encoding Data Language-Specific Formats JSON, XML, and Binary Variants Thrift and Protocol Buffers Avro The Merits of Schemas Modes of Dataflow Dataflow Through Databases Dataflow Through Services: REST and RPC Message-Passing Dataflow Summary Highlights and exerpts: …rolling upgrades allow new versions of a service to be released without downtime, thus encouraging frequent small releases over rare big releases and make deployments less risky — allowing faulty releases to be detected and rolled back before they affect a large number of users. Several data encoding formats were discussed:\nlanguage specific encoding textual formats like XML, JSON or CSV binary schema-driven formats like Avro, Thrift or Protocol Buffers allow quick and efficient encoding Several modes of dataflow:\ndatabase — communication between processes happens by writing and reading from db, probably the most simple approach RPC and REST APIs asynchronous message passing using message brokers or actors, e.g. RabbitMQ II. Distributed Data\n5. Replication Leaders and Followers Synchronous Versus Asynchronous Replication Setting Up New Followers Handling Node Outages Implementation of Replication Logs Problems with Replication Lag Reading Your Own Writes Monotonic Reads Consistent Prefix Reads Solutions for Replication Lag Multi-Leader Replication Use Cases for Multi-Leader Replication Handling Write Conflicts Multi-Leader Replication Topologies Leaderless Replication Writing to the Database When a Node Is Down Limitations of Quorum Consistency Sloppy Quorums and Hinted Handoff Detecting Concurrent Writes Summary Highlights and exerpts: Replication can serve several puposes:\nhigh availability disconnect operation latency scalability Goal of replication roughly speaking is very simple — keep same data in different locations. Despite simple description, we must think about concurrency issues, unavailable nodes and network interruptions.\nThree main approaches to replication:\nsingle-leader replication multi-leader replication leaderless replication 6. Partitioning Partitioning and Replication Partitioning of Key-Value Data Partitioning by Key Range Partitioning by Hash of Key Skewed Workloads and Relieving Hot Spots Partitioning and Secondary Indexes Partitioning Secondary Indexes by Document Partitioning Secondary Indexes by Term Rebalancing Partitions Strategies for Rebalancing Operations: Automatic or Manual Rebalancing Request Routing Parallel Query Execution Summary Highlights and exerpts: Partitioning is necessary when you have so much data that storing and processing it on a single machine is no longer feasible. The goal of partitioning is to spread the data and query load evenly across multiple machines, avoiding “hot spots” — nodes with disproportionately high load.\nThere are two main approaches to partitioning:\nkey range partitioning, this is when keys are sorted and each partition owns a specific set of those keys hash partitioning, where a hash function is applied to each key and a partition owns a range of hashes There are a lot of techniques for routing queries to the appropriate partition, from simple partition-aware load balancing to sophisticated parallel query execution engines.\n7. Transactions The Slippery Concept of a Transaction The Meaning of ACID Single-Object and Multi-Object Operations Weak Isolation Levels Read Committed Snapshot Isolation and Repeatable Read Preventing Lost Updates Write Skew and Phantoms Serializability Actual Serial Execution Two-Phase Locking (2PL) Serializable Snapshot Isolation (SSI) Summary Highlights and exerpts: Transactions are an abstraction layer that allows an application to pretend that certain concurrency problems and certain kinds of hardware ans software faults don’t exists. A large class of errors is reduced down to a simple transaction abort and the applications just needs to try again.\nSeveral widely used isolation levels are:\n“read committed” “snapshot isolation”, a.k.a. “repeatable read” “serializable” These isolation levels were discussed with following race conditions:\ndirty reads dirty writes read skew lost updates write skew phantom reads Only serializable isolation protects against all of these issues.\n8. The Trouble with Distributed Systems Faults and Partial Failures Cloud Computing and Supercomputing Unreliable Networks Network Faults in Practice Detecting Faults Timeouts and Unbounded Delays Synchronous Versus Asynchronous Networks Unreliable Clocks Monotonic Versus Time-of-Day Clocks Clock Synchronization and Accuracy Relying on Synchronized Clocks Process Pauses Knowledge, Truth, and Lies The Truth Is Defined by the Majority Byzantine Faults System Model and Reality Summary Highlights and exerpts: A wide range of problems with distributed system were discussed:\nunreliable network, message that is sent from one service to another might be lost or significantly delayed node’s clock may be significantly out of sync so you can’t rely on node’s time a process may pause for a significant amount of time and other nodes might decide it is dead and evict it, while node with paused process comes back to life : D All these problems is an integral part of distributed system and we should strive to build tolerance of partial failures into software so that the system as a whole may function even when some parts of it are not available.\nIn order to tolerate faults we should detect them first but it is complicated so most systems rely on a simple timeout to detect state of the node. Once failed node detected we have a pile of other problems and questions but, briefly, we need a kind of quorum/agreement between a number of nodes on what to do with failed node.\nIf you can avoid building distributed system it is strongly advised to do so.\nThere is also a question if it is possible to give hard real-time response guarantees and bounded delays in network and the answer is yer but it will be very expensive.\n9. Consistency and Consensus Consistency Guarantees Linearizability What Makes a System Linearizable? Relying on Linearizability Implementing Linearizable Systems The Cost of Linearizability Ordering Guarantees Ordering and Causality Sequence Number Ordering Total Order Broadcast Distributed Transactions and Consensus Atomic Commit and Two-Phase Commit (2PC) Distributed Transactions in Practice Fault-Tolerant Consensus Membership and Coordination Services Summary Highlights and exerpts: Linearizability — a popular consistency model. It is goal is to make replicated data appear as though there were only a single copy and to make all operations act on it atomically. This model tends to be slow, especially in environment with large network delays.\nAlso important concept was explored — causality, which imposes an ordering on events in the system (what happened before what, besed on cause and effect). But consistency model is not always work and that led us to consensus.\nAchieving consensus means deciding something is such a way that all nodes agree on what was decided and such that the decision is irevocable.\nNotes:\nDistributed transaction is database transaction that involve more that one node.\nThe most common algorithm for distrinbuted transactions is two-phase commit (2PC), it is implemented in many databases, message systems and application serivers. Exists better consensus algorigthms, such as Zab(ZooKeeper) and Raft(etcd — distributed key-value storage).\nIII. Derived Data\n10. Batch Processing\nBatch Processing with Unix Tools Simple Log Analysis The Unix Philosophy MapReduce and Distributed Filesystems MapReduce Job Execution Reduce-Side Joins and Grouping Map-Side Joins The Output of Batch Workflows Comparing Hadoop to Distributed Databases Beyond MapReduce Materialization of Intermediate State Graphs and Iterative Processing High-Level APIs and Languages Summary 11. Stream Processing\nTransmitting Event Streams Messaging Systems Partitioned Logs Databases and Streams Keeping Systems in Sync Change Data Capture Event Sourcing State, Streams, and Immutability Processing Streams Uses of Stream Processing Reasoning About Time Stream Joins Fault Tolerance Summary 12. The Future of Data Systems\nData Integration Combining Specialized Tools by Deriving Data Batch and Stream Processing Unbundling Databases Composing Data Storage Technologies Designing Applications Around Dataflow Observing Derived State Aiming for Correctness The End-to-End Argument for Databases Enforcing Constraints Timeliness and Integrity Trust, but Verify Doing the Right Thing Predictive Analytics Privacy and Tracking Summary Glossary\nIndex\nResourse:\nDesign Data Intesive Applications\n","wordCount":"2142","inLanguage":"en","datePublished":"2020-05-08T00:00:00Z","dateModified":"2020-05-08T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://slash3b.github.io/posts/2020-05-08-design-data-intensive-applications/"},"publisher":{"@type":"Organization","name":"Random notes to myself","logo":{"@type":"ImageObject","url":"https://slash3b.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://slash3b.github.io accesskey=h title="Random notes to myself (Alt + H)">Random notes to myself</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems</h1><div class=post-meta><span title='2020-05-08 00:00:00 +0000 UTC'>May 8, 2020</span></div></header><div class=post-content><p>Designing Data-Intensive Applications by Martin Kleppmann is an amazing journey into the world of distributed systems. It is a must read, especially if you are like me — self-taught programmer. I think it will be usefull even if you do not plan to work with distributed systems, it is just interesting per se.<br>I really learned a lot, than you Martin!</p><p>I have a feeling that I managed to digest and remember around 5% of the information, and the book contains a lot of good stuff, I mean <strong>a lot</strong>. So for myself I decided to get table of book contents and augment it with little hints and notes that will be helpful in case I want to remember something quickly.</p><blockquote><p>Computing is pop culture. [&mldr;] Pop culture holds a disdain for history. Pop culture is all about identity and feeling like you&rsquo;re participating. It has nothing to do with cooperation, the past or the furute — it&rsquo;s living in the present. I think the same is true of most people who write code for money. They have no idea where [their culture came from].
Alan Kay, in interview with Dr. Dobb&rsquo;s journal(2012)</p></blockquote><p><strong>Table of Contents</strong></p><p><strong>I. Foundations of Data Systems</strong></p><ul><li>1. Reliable, Scalable, and Maintainable Applications<ul><li>Thinking About Data Systems</li><li>Reliability<ul><li>Hardware Faults</li><li>Software Errors</li><li>Human Errors</li><li>How Important Is Reliability?</li></ul></li><li>Scalability<ul><li>Describing Load</li><li>Describing Performance</li><li>Approaches for Coping with Load</li></ul></li><li>Maintainability<ul><li>Operability: Making Life Easy for Operations</li><li>Simplicity: Managing Complexity</li><li>Evolvability: Making Change Easy</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts>#</a></h4><p>Many applications today are data-intensive and not compute-intensive. Scalability is the term we use to describe a system&rsquo;s ability to cope with increased load. Load paramenters — e.g. requests per second, number of simultaneous user online, the ration of reads to writes in a database. Percentiles — e.g. with a sorted list of responses, you can see that if the 95th percentile response time is 1.5 seconds, that means 95 out of 100 requests take less than 1.5 seconds and 5 out of 100 requests take 1.5 seconds or more. Percentiles are used in Service Level Objectives and Service Level Agreements.</p><ul><li>Making a system simpler does not necessarily mean reducing its functionality; it can also mean removing accidental complexity. Reliability means making systems work correctly, even when faults occur.</li><li>Scalability means having strategies for keeping performance good, even when load increases.</li><li>Maintainability it&rsquo;s about making life better for the engineering and operations teams who need to work with the system.</li></ul><hr><ul><li>2. Data Models and Query Languages<ul><li>Relational Model Versus Document Model<ul><li>The Birth of NoSQL</li><li>The Object-Relational Mismatch</li><li>Many-to-One and Many-to-Many Relationships</li><li>Are Document Databases Repeating History?</li><li>Relational Versus Document Databases Today</li></ul></li><li>Query Languages for Data<ul><li>Declarative Queries on the Web</li><li>MapReduce Querying</li></ul></li><li>Graph-Like Data Models<ul><li>Property Graphs</li><li>The Cypher Query Language</li><li>Graph Queries in SQL</li><li>Triple-Stores and SPARQL</li><li>The Foundation: Datalog</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts-1>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts-1>#</a></h4><p>Historically data was represented as a tree. Later on with neccessity to represent relations between records relation model was invented. Today we have tree &ldquo;main directions&rdquo; to store data:</p><ul><li>&ldquo;relational&rdquo; — in case you have a relations between records</li><li>&ldquo;graph&rdquo; covers cases when everything is related to everything, think Twitter connections between people</li><li>&ldquo;document&rdquo; good when you have a fewer relations between documents or none at all</li></ul><hr><ul><li>3. Storage and Retrieval<ul><li>Data Structures That Power Your Database<ul><li>Hash Indexes</li><li>SSTables and LSM-Trees</li><li>B-Trees</li><li>Comparing B-Trees and LSM-Trees</li><li>Other Indexing Structures</li></ul></li><li>Transaction Processing or Analytics?<ul><li>Data Warehousing</li><li>Stars and Snowflakes: Schemas for Analytics</li></ul></li><li>Column-Oriented Storage<ul><li>Column Compression</li><li>Sort Order in Column Storage</li><li>Writing to Column-Oriented Storage</li><li>Aggregation: Data Cubes and Materialized Views</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts-2>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts-2>#</a></h4><p>This chapter tries to explain how databases handle storage and retrieval.</p><p>Storage engines can be divided in two categories:</p><ul><li>optimized for transaction processing — OLTP, a.k.a. Online Transaction Processing. This is where typical CRUD lives and thrives.</li><li>optimized for analytics — OLAP, a.k.a. Online Analytical Processing.</li></ul><p>OLTP can be divided in two sub-categories:</p><ul><li>log-structured append-only model with clean up of obsolete files. Examples: Bitcask, SSTables, LSM-trees, LevelDB,
Cassandra, HBase, Lucene and etc.</li><li>update-in-place model, which treats the disk as a set of fixed-size pages than can be updated.</li></ul><p>Log-structured storage engines are a comparatively recent development. Their key idea is that they systematically turn
random-access writes into sequential writes on disk, which enables higher write throughtput due to performance
characteristics of hard drives and SSDs.</p><h4 id=other-notes>Other notes:<a hidden class=anchor aria-hidden=true href=#other-notes>#</a></h4><p>Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.</p><p>A database index is an additional structure, traditionally — B-tree, that has to be updated when we write to the
database. Hence we have a tradeoff here, namely — index speed up read requests but slows down write requests.<br>Bitcast is the default storage engine of Riak, it is roughtly an append-only storage that tracks every record offset.
Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.</p><p>It &mldr; seems likely that in ther foreseeable future relational databases will continue to be used alongside a broad viriety of nonrelational datastore — an idea that is called <em>polyglot persistence</em>.<br>&mldr; However if your application does use many-to-many relations, the document model becomes less appealing.<br>When you most likely have many interconnected entities, where everything migh relate to everything — graph database is a good choice.</p><hr><ul><li>4. Encoding and Evolution<ul><li>Formats for Encoding Data<ul><li>Language-Specific Formats</li><li>JSON, XML, and Binary Variants</li><li>Thrift and Protocol Buffers</li><li>Avro</li><li>The Merits of Schemas</li></ul></li><li>Modes of Dataflow<ul><li>Dataflow Through Databases</li><li>Dataflow Through Services: REST and RPC</li><li>Message-Passing Dataflow</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts-3>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts-3>#</a></h4><p>&mldr;rolling upgrades allow new versions of a service to be released without downtime, thus encouraging frequent small releases over rare big releases and make deployments less risky — allowing faulty releases to be detected and rolled back before they affect a large number of users.<br>Several data encoding formats were discussed:</p><ul><li>language specific encoding</li><li>textual formats like XML, JSON or CSV</li><li>binary schema-driven formats like Avro, Thrift or Protocol Buffers allow quick and efficient encoding</li></ul><p>Several modes of dataflow:</p><ul><li>database — communication between processes happens by writing and reading from db, probably the most simple approach</li><li>RPC and REST APIs</li><li>asynchronous message passing using message brokers or actors, e.g. RabbitMQ</li></ul><hr><p><strong>II. Distributed Data</strong></p><ul><li>5. Replication<ul><li>Leaders and Followers<ul><li>Synchronous Versus Asynchronous Replication</li><li>Setting Up New Followers</li><li>Handling Node Outages</li><li>Implementation of Replication Logs</li></ul></li><li>Problems with Replication Lag<ul><li>Reading Your Own Writes</li><li>Monotonic Reads</li><li>Consistent Prefix Reads</li><li>Solutions for Replication Lag</li></ul></li><li>Multi-Leader Replication<ul><li>Use Cases for Multi-Leader Replication</li><li>Handling Write Conflicts</li><li>Multi-Leader Replication Topologies</li></ul></li><li>Leaderless Replication<ul><li>Writing to the Database When a Node Is Down</li><li>Limitations of Quorum Consistency</li><li>Sloppy Quorums and Hinted Handoff</li><li>Detecting Concurrent Writes</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts-4>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts-4>#</a></h4><p>Replication can serve several puposes:</p><ul><li>high availability</li><li>disconnect operation</li><li>latency</li><li>scalability</li></ul><p>Goal of replication roughly speaking is very simple — keep same data in different locations. Despite simple description,
we must think about concurrency issues, unavailable nodes and network interruptions.</p><p>Three main approaches to replication:</p><ul><li>single-leader replication</li><li>multi-leader replication</li><li>leaderless replication</li></ul><hr><ul><li>6. Partitioning<ul><li>Partitioning and Replication</li><li>Partitioning of Key-Value Data<ul><li>Partitioning by Key Range</li><li>Partitioning by Hash of Key</li><li>Skewed Workloads and Relieving Hot Spots</li></ul></li><li>Partitioning and Secondary Indexes<ul><li>Partitioning Secondary Indexes by Document</li><li>Partitioning Secondary Indexes by Term</li></ul></li><li>Rebalancing Partitions<ul><li>Strategies for Rebalancing</li><li>Operations: Automatic or Manual Rebalancing</li></ul></li><li>Request Routing<ul><li>Parallel Query Execution</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts-5>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts-5>#</a></h4><p>Partitioning is necessary when you have so much data that storing and processing it on a single machine is no longer
feasible. The goal of partitioning is to spread the data and query load evenly across multiple machines, avoiding &ldquo;hot
spots&rdquo; — nodes with disproportionately high load.</p><p>There are two main approaches to partitioning:</p><ul><li>key range partitioning, this is when keys are sorted and each partition owns a specific set of those keys</li><li>hash partitioning, where a hash function is applied to each key and a partition owns a range of hashes</li></ul><p>There are a lot of techniques for routing queries to the appropriate partition, from simple partition-aware
load balancing to sophisticated parallel query execution engines.</p><hr><ul><li>7. Transactions<ul><li>The Slippery Concept of a Transaction<ul><li>The Meaning of ACID</li><li>Single-Object and Multi-Object Operations</li></ul></li><li>Weak Isolation Levels<ul><li>Read Committed</li><li>Snapshot Isolation and Repeatable Read</li><li>Preventing Lost Updates</li><li>Write Skew and Phantoms</li></ul></li><li>Serializability<ul><li>Actual Serial Execution</li><li>Two-Phase Locking (2PL)</li><li>Serializable Snapshot Isolation (SSI)</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts-6>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts-6>#</a></h4><p>Transactions are an abstraction layer that allows an application to pretend that certain concurrency problems and
certain kinds of hardware ans software faults don&rsquo;t exists. A large class of errors is reduced down to a simple
<em>transaction abort</em> and the applications just needs to try again.</p><p>Several widely used isolation levels are:</p><ul><li>&ldquo;read committed&rdquo;</li><li>&ldquo;snapshot isolation&rdquo;, a.k.a. &ldquo;repeatable read&rdquo;</li><li>&ldquo;serializable&rdquo;</li></ul><p>These isolation levels were discussed with following race conditions:</p><ul><li>dirty reads</li><li>dirty writes</li><li>read skew</li><li>lost updates</li><li>write skew</li><li>phantom reads</li></ul><p>Only serializable isolation protects against all of these issues.</p><hr><ul><li>8. The Trouble with Distributed Systems<ul><li>Faults and Partial Failures<ul><li>Cloud Computing and Supercomputing</li></ul></li><li>Unreliable Networks<ul><li>Network Faults in Practice</li><li>Detecting Faults</li><li>Timeouts and Unbounded Delays</li><li>Synchronous Versus Asynchronous Networks</li></ul></li><li>Unreliable Clocks<ul><li>Monotonic Versus Time-of-Day Clocks</li><li>Clock Synchronization and Accuracy</li><li>Relying on Synchronized Clocks</li><li>Process Pauses</li></ul></li><li>Knowledge, Truth, and Lies<ul><li>The Truth Is Defined by the Majority</li><li>Byzantine Faults</li><li>System Model and Reality</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts-7>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts-7>#</a></h4><p>A wide range of problems with distributed system were discussed:</p><ul><li>unreliable network, message that is sent from one service to another might be lost or significantly delayed</li><li>node&rsquo;s clock may be significantly out of sync so you can&rsquo;t rely on node&rsquo;s time</li><li>a process may pause for a significant amount of time and other nodes might decide it is dead and evict it, while node
with paused process comes back to life : D</li></ul><p>All these problems is an integral part of distributed system and we should strive to build tolerance of partial failures
into software so that the system as a whole may function even when some parts of it are not available.</p><p>In order to tolerate faults we should detect them first but it is complicated so most systems rely on a simple timeout
to detect state of the node. Once failed node detected we have a pile of other problems and questions but, briefly, we
need a kind of quorum/agreement between a number of nodes on what to do with failed node.</p><p>If you can avoid building distributed system it is strongly advised to do so.</p><p>There is also a question if it is possible to give hard real-time response guarantees and bounded delays in network and
the answer is yer but it will be very expensive.</p><hr><ul><li>9. Consistency and Consensus<ul><li>Consistency Guarantees</li><li>Linearizability<ul><li>What Makes a System Linearizable?</li><li>Relying on Linearizability</li><li>Implementing Linearizable Systems</li><li>The Cost of Linearizability</li></ul></li><li>Ordering Guarantees<ul><li>Ordering and Causality</li><li>Sequence Number Ordering</li><li>Total Order Broadcast</li></ul></li><li>Distributed Transactions and Consensus<ul><li>Atomic Commit and Two-Phase Commit (2PC)</li><li>Distributed Transactions in Practice</li><li>Fault-Tolerant Consensus</li><li>Membership and Coordination Services</li></ul></li><li>Summary</li></ul></li></ul><h4 id=highlights-and-exerpts-8>Highlights and exerpts:<a hidden class=anchor aria-hidden=true href=#highlights-and-exerpts-8>#</a></h4><p>Linearizability — a popular consistency model. It is goal is to make replicated data appear as though there were only a
single copy and to make all operations act on it atomically. This model tends to be slow, especially in environment with
large network delays.</p><p>Also important concept was explored — causality, which imposes an ordering on events in the system (what happened before
what, besed on cause and effect). But consistency model is not always work and that led us to <em>consensus</em>.</p><p>Achieving consensus means deciding something is such a way that all nodes agree on what was decided and such that the
decision is irevocable.</p><p>Notes:<br>Distributed transaction is database transaction that involve more that one node.<br>The most common algorithm for distrinbuted transactions is <em>two-phase commit</em> (2PC), it is implemented in many
databases, message systems and application serivers. Exists better consensus algorigthms, such as Zab(ZooKeeper) and
<a href=http://thesecretlivesofdata.com/raft/>Raft</a>(etcd — distributed key-value storage).</p><hr><p><strong>III. Derived Data</strong></p><ul><li><p>10. Batch Processing</p><ul><li>Batch Processing with Unix Tools<ul><li>Simple Log Analysis</li><li>The Unix Philosophy</li></ul></li><li>MapReduce and Distributed Filesystems<ul><li>MapReduce Job Execution</li><li>Reduce-Side Joins and Grouping</li><li>Map-Side Joins</li><li>The Output of Batch Workflows</li><li>Comparing Hadoop to Distributed Databases</li></ul></li><li>Beyond MapReduce<ul><li>Materialization of Intermediate State</li><li>Graphs and Iterative Processing</li><li>High-Level APIs and Languages</li></ul></li><li>Summary</li></ul></li><li><p>11. Stream Processing</p><ul><li>Transmitting Event Streams<ul><li>Messaging Systems</li><li>Partitioned Logs</li></ul></li><li>Databases and Streams<ul><li>Keeping Systems in Sync</li><li>Change Data Capture</li><li>Event Sourcing</li><li>State, Streams, and Immutability</li></ul></li><li>Processing Streams<ul><li>Uses of Stream Processing</li><li>Reasoning About Time</li><li>Stream Joins</li><li>Fault Tolerance</li></ul></li><li>Summary</li></ul></li><li><p>12. The Future of Data Systems</p><ul><li>Data Integration<ul><li>Combining Specialized Tools by Deriving Data</li><li>Batch and Stream Processing</li></ul></li><li>Unbundling Databases<ul><li>Composing Data Storage Technologies</li><li>Designing Applications Around Dataflow</li><li>Observing Derived State</li></ul></li><li>Aiming for Correctness<ul><li>The End-to-End Argument for Databases</li><li>Enforcing Constraints</li><li>Timeliness and Integrity</li><li>Trust, but Verify</li></ul></li><li>Doing the Right Thing<ul><li>Predictive Analytics</li><li>Privacy and Tracking</li></ul></li><li>Summary</li></ul></li><li><p>Glossary</p></li><li><p>Index</p></li></ul><p>Resourse:<br><a href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable-ebook/dp/B06XPJML5D/ref=sr_1_1?crid=24WFQ31Q3MIOA&dchild=1&keywords=design+data+intensive+applications&qid=1585083016&sprefix=design+data+in%2Caps%2C277&sr=8-1">Design Data Intesive Applications</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://slash3b.github.io>Random notes to myself</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>